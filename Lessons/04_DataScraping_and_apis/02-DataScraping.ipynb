{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## all imports\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import urllib\n",
    "import bs4 #this is beautiful soup\n",
    "import time\n",
    "import operator\n",
    "import socket\n",
    "import re # regular expressions\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "#from secret import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "API registrations\n",
    "=================\n",
    "\n",
    "If you would like to run all the examples in this notebook, you need to register for the following APIs:\n",
    "\n",
    "* Twitter\n",
    "\n",
    "https://apps.twitter.com/app/new\n",
    "\n",
    "* Twitter instructions\n",
    "\n",
    "https://twittercommunity.com/t/how-to-get-my-api-key/7033"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Todays lecture:\n",
    "===============\n",
    "\n",
    "* all about data scraping\n",
    "* ***What is it? ***\n",
    "* How to do it:\n",
    "    - from a website\n",
    "    - with an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Answer: Data scraping is about obtaining data from webpages. There is low level scraping where you parse the data out of the html code of the webpage. There also is scraping over APIs from websites who try to make your life a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "IPython Notebooks:\n",
    "===================\n",
    "\n",
    "![IPython](images/ipython.png \"IPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "General advice about programming\n",
    "==================================\n",
    "\n",
    "* You will find nearly everything on google\n",
    "* Try: length of a list in python\n",
    "* A programmer is someone who can turn stack overflow snippets into running code\n",
    "* Use tab completion\n",
    "* Make your variable names meaningful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Python data scraping\n",
    "====================\n",
    "\n",
    "* Why scrape the web?\n",
    "    - vast source of information\n",
    "    - automate tasks\n",
    "    - keep up with sites\n",
    "    - fun!\n",
    "\n",
    "** Can you think of examples ? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Python data scraping\n",
    "====================\n",
    "\n",
    "* copyrights and permission:\n",
    "    - be careful and polite\n",
    "    - give credit\n",
    "    - care about media law\n",
    "    - don't be evil (no spam, overloading sites, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Robots.txt\n",
    "==========\n",
    "\n",
    "![Robots.txt](images/robots_txt.jpg \"Robots.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Robots.txt\n",
    "==========\n",
    "\n",
    "* specified by web site owner\n",
    "* gives instructions to web robots (aka your script)\n",
    "* is located at the top-level directory of the web server\n",
    "\n",
    "http://www.example.com/robots.txt\n",
    "\n",
    "If you want you can also have a look at\n",
    "\n",
    "http://google.com/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Robots.txt\n",
    "==========\n",
    "\n",
    "*** What does this one do? ***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "User-agent: Google\n",
    "Disallow:\n",
    "\n",
    "User-agent: *\n",
    "Disallow: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Answer: This file allows google to search through everything on the server, while all others should stay completely away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Things to consider:\n",
    "-------------------\n",
    "\n",
    "* can be just ignored\n",
    "* can be a security risk - *** Why? ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Answer: You are basically telling everybody who cares to look into the file where you have stored sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scraping with Python:\n",
    "=====================\n",
    "\n",
    "* scraping is all about HTML tags\n",
    "* bad news: \n",
    "    - need to learn about tags\n",
    "    - websites can be ugly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HTML\n",
    "=====\n",
    "\n",
    "* HyperText Markup Language\n",
    "\n",
    "* standard for creating webpages\n",
    "\n",
    "* HTML tags \n",
    "    - have angle brackets\n",
    "    - typically come in pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is an example for a minimal webpage defined in HTML tags. The root tag is `<html>` and then you have the `<head>` tag. This part of the page typically includes the title of the page and might also have other meta information like the author or keywords that are important for search engines. The `<body>` tag marks the actual content of the page. You can play around with the `<h2>` tag trying different header levels. They range from 1 to 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "  <head>\n",
       "    <title>This is a title</title>\n",
       "  </head>\n",
       "  <body>\n",
       "    <h2> Test </h2>\n",
       "    <p>Hello world!</p>\n",
       "  </body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmlString = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>This is a title</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2> Test </h2>\n",
    "    <p>Hello world!</p>\n",
    "  </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "htmlOutput = HTML(htmlString)\n",
    "htmlOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Useful Tags\n",
    "===========\n",
    "\n",
    "* heading\n",
    "`<h1></h1> ... <h6></h6>`\n",
    "\n",
    "* paragraph\n",
    "`<p></p>` \n",
    "\n",
    "* line break\n",
    "`<br>` \n",
    "\n",
    "* link with attribute\n",
    "\n",
    "`<a href=\"http://www.example.com/\">An example link</a>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scraping with Python:\n",
    "=====================\n",
    "\n",
    "* example of a beautifully simple webpage:\n",
    "\n",
    "http://www.crummy.com/software/BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scraping with Python:\n",
    "=====================\n",
    "\n",
    "* good news: \n",
    "    - some browsers help\n",
    "    - look for: inspect element\n",
    "    - need only basic html\n",
    "    \n",
    "** Try 'Ctrl-Shift I' in Chrome **\n",
    "\n",
    "** Try 'Command-Option I' in Safari **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scraping with Python\n",
    "==================\n",
    "\n",
    "* different useful libraries:\n",
    "    - urllib\n",
    "    - beautifulsoup\n",
    "    - pattern\n",
    "    - soupy\n",
    "    - LXML\n",
    "    - Selenium\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The following cell just defines a url as a string and then reads the data from that url using the `urllib` library. If you uncomment the print command you see that we got the whole HTML content of the page into the string variable source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\\n\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\\n<html>\\n<head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\\n<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\\n<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/nb/themes/Default/nb.css\">\\n<meta name=\"Description\" content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\">\\n<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\\n<meta name=\"author\" content=\"Leonard Richardson\">\\n</head>\\n<body bgcolor=\"white\" text=\"black\" link=\"blue\" vlink=\"660066\" alink=\"red\">\\n<img align=\"right\" src=\"10.1.jpg\" width=\"250\"><br />\\n\\n<p>You didn\\'t write that awful page. You\\'re just trying to get some\\ndata out of it. Beautiful Soup is here to help. Since 2004, it\\'s been\\nsaving programmers hours or days of work on quick-turnaround\\nscreen scraping projects.</p>\\n\\n<div align=\"center\">\\n\\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\\n\\n<p>\"A tremendous boon.\" -- Python411 Podcast</p>\\n\\n<p>[ <a href=\"#Download\">Download</a> | <a\\nhref=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\\n\\n<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\n<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</small></p>\\n\\n\\n\\n</div>\\n\\n<p><i>If you have questions, send them to <a\\nhref=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\\n\\n<p>Beautiful Soup is a Python library designed for quick turnaround\\nprojects like screen-scraping. Three features make it powerful:\\n\\n<ol>\\n\\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\ndissecting a document and extracting what you need. It doesn\\'t take\\nmuch code to write an application\\n\\n<li>Beautiful Soup automatically converts incoming documents to\\nUnicode and outgoing documents to UTF-8. You don\\'t have to think\\nabout encodings, unless the document doesn\\'t specify an encoding and\\nBeautiful Soup can\\'t detect one. Then you just have to specify the\\noriginal encoding.\\n\\n<li>Beautiful Soup sits on top of popular Python parsers like <a\\nhref=\"http://lxml.de/\">lxml</a> and <a\\nhref=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\\nto try out different parsing strategies or trade speed for\\nflexibility.\\n\\n</ol>\\n\\n<p>Beautiful Soup parses anything you give it, and does the tree\\ntraversal stuff for you. You can tell it \"Find all the links\", or\\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\\nlinks whose urls match \"foo.com\", or \"Find the table heading that\\'s\\ngot bold text, then give me that text.\"\\n\\n<p>Valuable data that was once locked up in poorly-designed websites\\nis now within your reach. Projects that would have taken hours take\\nonly minutes with Beautiful Soup.\\n\\n<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\\n\\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\\n\\n<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\\n4.8.0</a> (July 20, 2019). You can install Beautiful Soup 4 with\\n<code>pip install beautifulsoup4</code>.\\n\\n<p>In Debian and Ubuntu, Beautiful Soup is available as the\\n<code>python-bs4</code> package (for Python 2) or the\\n<code>python3-bs4</code> package (for Python 3). In Fedora it\\'s\\navailable as the <code>python-beautifulsoup4</code> package.\\n\\n<p>Beautiful Soup is licensed under the MIT license, so you can also\\ndownload the tarball, drop the <code>bs4/</code> directory into almost\\nany Python application (or into your library path) and start using it\\nimmediately. (If you want to do this under Python 3, you will need to\\nmanually convert the code using <code>2to3</code>.)\\n\\n<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\\n\\n<h3>Beautiful Soup 3</h3>\\n\\n<p>Beautiful Soup 3 was the official release line of Beautiful Soup\\nfrom May 2006 to March 2012. It is considered stable, and only\\ncritical security bugs will be fixed. <a\\nhref=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here\\'s\\nthe Beautiful Soup 3 documentation.</a>\\n\\n<p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\\nthe same license as Python itself.\\n\\n<p>The current release of Beautiful Soup 3 is <a\\nhref=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\\n2012). You can install Beautiful Soup 3 with <code>pip install\\nBeautifulSoup</code>. It\\'s also available as\\n<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\\n<code>python-BeautifulSoup</code> in Fedora.\\n\\n<p>You can also download the tarball and use\\n<code>BeautifulSoup.py</code> in your project directly.\\n\\n\\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\\n\\n<p>Over the years, Beautiful Soup has been used in hundreds of\\ndifferent projects. There\\'s no way I can list them all, but I want to\\nhighlight a few high-profile projects. Beautiful Soup isn\\'t what makes\\nthese projects interesting, but it did make their completion easier:\\n\\n<ul>\\n\\n<li><a\\n href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>, a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\n<li>Reddit uses Beautiful Soup to <a\\nhref=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\na page that\\'s been linked to and find a representative image</a>.\\n\\n<li>Alexander Harrowell uses Beautiful Soup to <a\\n href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a> of an arms merchant.\\n\\n<li>The developers of Python itself used Beautiful Soup to <a\\nhref=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\nbug tracker from Sourceforge to Roundup</a>.\\n\\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\nuses Beautiful Soup to <A\\nhref=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\nstatewide election results</a>.\\n\\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a> uses Beautiful Soup in <a\\nhref=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n</ul>\\n\\n<p>If you\\'ve used Beautiful Soup in a project you\\'d like me to know\\nabout, please do send email to me or <a\\nhref=\"http://groups.google.com/group/beautifulsoup/\">the discussion\\ngroup</a>.\\n\\n<h2>Development</h2>\\n\\n<p>Development happens at <a\\nhref=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a\\nhref=\"https://code.launchpad.net/beautifulsoup/\">get the source\\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\nbugs</a>.<hr><table><tr><td valign=\"top\">\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Sunday, July 21 2019, 17:41:10 Nowhere Standard Time and last built on Tuesday, September 17 2019, 23:00:01 Nowhere Standard Time.</p><p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"></a></td><td valign=\"top\">Crummy is &copy; 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table></span><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></p></td><td valign=top><p><b>Document tree:</b>\\n<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dl>\\n</dl>\\n</dl>\\n\\n\\nSite Search:\\n\\n<form method=\"get\" action=\"/search/\">\\n        <input type=\"text\" name=\"q\" maxlength=\"255\" value=\"\"></input>\\n        </form>\\n        </td>\\n\\n</tr>\\n\\n</table>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "url = 'https://www.crummy.com/software/BeautifulSoup'\n",
    "source = urlopen(url).read()\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = str(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\\'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\"\\\\n\"http://www.w3.org/TR/REC-html40/transitional.dtd\">\\\\n<html>\\\\n<head>\\\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\\\\n<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\\\\n<link rev=\"made\" href=\"mailto:leonardr@segfault.org\">\\\\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/nb/themes/Default/nb.css\">\\\\n<meta name=\"Description\" content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\">\\\\n<meta name=\"generator\" content=\"Markov Approximation 1.4 (module: leonardr)\">\\\\n<meta name=\"author\" content=\"Leonard Richardson\">\\\\n</head>\\\\n<body bgcolor=\"white\" text=\"black\" link=\"blue\" vlink=\"660066\" alink=\"red\">\\\\n<img align=\"right\" src=\"10.1.jpg\" width=\"250\"><br />\\\\n\\\\n<p>You didn\\\\\\'t write that awful page. You\\\\\\'re just trying to get some\\\\ndata out of it. Beautiful Soup is here to help. Since 2004, it\\\\\\'s been\\\\nsaving programmers hours or days of work on quick-turnaround\\\\nscreen scraping projects.</p>\\\\n\\\\n<div align=\"center\">\\\\n\\\\n<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>\\\\n\\\\n<p>\"A tremendous boon.\" -- Python411 Podcast</p>\\\\n\\\\n<p>[ <a href=\"#Download\">Download</a> | <a\\\\nhref=\"bs4/doc/\">Documentation</a> | <a href=\"#HallOfFame\">Hall of Fame</a> | <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a> | <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a> | <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>  | <a href=\"zine/\">Zine</a> ]</p>\\\\n\\\\n<p><small>If you use Beautiful Soup as part of your work, please consider a <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website\">Tidelift subscription</a>. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\\\\n<p>If Beautiful Soup is useful to you on a personal level, you might like to read <a href=\"zine/\"><i>Tool Safety</i></a>, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!</small></p>\\\\n\\\\n\\\\n\\\\n</div>\\\\n\\\\n<p><i>If you have questions, send them to <a\\\\nhref=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">the discussion\\\\ngroup</a>. If you find a bug, <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>.</i></p>\\\\n\\\\n<p>Beautiful Soup is a Python library designed for quick turnaround\\\\nprojects like screen-scraping. Three features make it powerful:\\\\n\\\\n<ol>\\\\n\\\\n<li>Beautiful Soup provides a few simple methods and Pythonic idioms\\\\nfor navigating, searching, and modifying a parse tree: a toolkit for\\\\ndissecting a document and extracting what you need. It doesn\\\\\\'t take\\\\nmuch code to write an application\\\\n\\\\n<li>Beautiful Soup automatically converts incoming documents to\\\\nUnicode and outgoing documents to UTF-8. You don\\\\\\'t have to think\\\\nabout encodings, unless the document doesn\\\\\\'t specify an encoding and\\\\nBeautiful Soup can\\\\\\'t detect one. Then you just have to specify the\\\\noriginal encoding.\\\\n\\\\n<li>Beautiful Soup sits on top of popular Python parsers like <a\\\\nhref=\"http://lxml.de/\">lxml</a> and <a\\\\nhref=\"http://code.google.com/p/html5lib/\">html5lib</a>, allowing you\\\\nto try out different parsing strategies or trade speed for\\\\nflexibility.\\\\n\\\\n</ol>\\\\n\\\\n<p>Beautiful Soup parses anything you give it, and does the tree\\\\ntraversal stuff for you. You can tell it \"Find all the links\", or\\\\n\"Find all the links of class <tt>externalLink</tt>\", or \"Find all the\\\\nlinks whose urls match \"foo.com\", or \"Find the table heading that\\\\\\'s\\\\ngot bold text, then give me that text.\"\\\\n\\\\n<p>Valuable data that was once locked up in poorly-designed websites\\\\nis now within your reach. Projects that would have taken hours take\\\\nonly minutes with Beautiful Soup.\\\\n\\\\n<p>Interested? <a href=\"bs4/doc/\">Read more.</a>\\\\n\\\\n<a name=\"Download\"><h2>Download Beautiful Soup</h2></a>\\\\n\\\\n<p>The current release is <a href=\"bs4/download/\">Beautiful Soup\\\\n4.8.0</a> (July 20, 2019). You can install Beautiful Soup 4 with\\\\n<code>pip install beautifulsoup4</code>.\\\\n\\\\n<p>In Debian and Ubuntu, Beautiful Soup is available as the\\\\n<code>python-bs4</code> package (for Python 2) or the\\\\n<code>python3-bs4</code> package (for Python 3). In Fedora it\\\\\\'s\\\\navailable as the <code>python-beautifulsoup4</code> package.\\\\n\\\\n<p>Beautiful Soup is licensed under the MIT license, so you can also\\\\ndownload the tarball, drop the <code>bs4/</code> directory into almost\\\\nany Python application (or into your library path) and start using it\\\\nimmediately. (If you want to do this under Python 3, you will need to\\\\nmanually convert the code using <code>2to3</code>.)\\\\n\\\\n<p>Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\\\\n\\\\n<h3>Beautiful Soup 3</h3>\\\\n\\\\n<p>Beautiful Soup 3 was the official release line of Beautiful Soup\\\\nfrom May 2006 to March 2012. It is considered stable, and only\\\\ncritical security bugs will be fixed. <a\\\\nhref=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">Here\\\\\\'s\\\\nthe Beautiful Soup 3 documentation.</a>\\\\n\\\\n<p>Beautiful Soup 3 works only under Python 2.x. It is licensed under\\\\nthe same license as Python itself.\\\\n\\\\n<p>The current release of Beautiful Soup 3 is <a\\\\nhref=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">3.2.1</a> (February 16,\\\\n2012). You can install Beautiful Soup 3 with <code>pip install\\\\nBeautifulSoup</code>. It\\\\\\'s also available as\\\\n<code>python-beautifulsoup</code> in Debian and Ubuntu, and as\\\\n<code>python-BeautifulSoup</code> in Fedora.\\\\n\\\\n<p>You can also download the tarball and use\\\\n<code>BeautifulSoup.py</code> in your project directly.\\\\n\\\\n\\\\n<a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>\\\\n\\\\n<p>Over the years, Beautiful Soup has been used in hundreds of\\\\ndifferent projects. There\\\\\\'s no way I can list them all, but I want to\\\\nhighlight a few high-profile projects. Beautiful Soup isn\\\\\\'t what makes\\\\nthese projects interesting, but it did make their completion easier:\\\\n\\\\n<ul>\\\\n\\\\n<li><a\\\\n href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\\\n Type\"</a>, a work of digital art on display in the lobby of the New\\\\n York Times building, uses Beautiful Soup to scrape news feeds.\\\\n\\\\n<li>Reddit uses Beautiful Soup to <a\\\\nhref=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">parse\\\\na page that\\\\\\'s been linked to and find a representative image</a>.\\\\n\\\\n<li>Alexander Harrowell uses Beautiful Soup to <a\\\\n href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\\\n activities</a> of an arms merchant.\\\\n\\\\n<li>The developers of Python itself used Beautiful Soup to <a\\\\nhref=\"http://svn.python.org/view/tracker/importer/\">migrate the Python\\\\nbug tracker from Sourceforge to Roundup</a>.\\\\n\\\\n<li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\\\nuses Beautiful Soup to <A\\\\nhref=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">gather\\\\nstatewide election results</a>.\\\\n\\\\n<li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\\\\\'s Forecast\\\\nApplications Branch</a> uses Beautiful Soup in <a\\\\nhref=\"http://laps.noaa.gov/topograbber/\">TopoGrabber</a>, a script for\\\\ndownloading \"high resolution USGS datasets.\"\\\\n\\\\n</ul>\\\\n\\\\n<p>If you\\\\\\'ve used Beautiful Soup in a project you\\\\\\'d like me to know\\\\nabout, please do send email to me or <a\\\\nhref=\"http://groups.google.com/group/beautifulsoup/\">the discussion\\\\ngroup</a>.\\\\n\\\\n<h2>Development</h2>\\\\n\\\\n<p>Development happens at <a\\\\nhref=\"https://launchpad.net/beautifulsoup\">Launchpad</a>. You can <a\\\\nhref=\"https://code.launchpad.net/beautifulsoup/\">get the source\\\\ncode</a> or <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\\\nbugs</a>.<hr><table><tr><td valign=\"top\">\\\\n<p>This document (<a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>) is part of Crummy, the webspace of <a href=\"/self/\">Leonard Richardson</a> (<a href=\"/self/contact.html\">contact information</a>). It was last modified on Sunday, July 21 2019, 17:41:10 Nowhere Standard Time and last built on Tuesday, September 17 2019, 23:00:01 Nowhere Standard Time.</p><p><table class=\"licenseText\"><tr><td><a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"></a></td><td valign=\"top\">Crummy is &copy; 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>.</td></tr></table></span><!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>--></p></td><td valign=top><p><b>Document tree:</b>\\\\n<dl><dd><a href=\"http://www.crummy.com/\">http://www.crummy.com/</a><dl><dd><a href=\"http://www.crummy.com/software/\">software/</a><dl><dd><a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a></dl>\\\\n</dl>\\\\n</dl>\\\\n\\\\n\\\\nSite Search:\\\\n\\\\n<form method=\"get\" action=\"/search/\">\\\\n        <input type=\"text\" name=\"q\" maxlength=\"255\" value=\"\"></input>\\\\n        </form>\\\\n        </td>\\\\n\\\\n</tr>\\\\n\\\\n</table>\\\\n</body>\\\\n</html>\\\\n\\''"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quiz :\n",
    "======\n",
    "\n",
    "* Is the word 'Alice' mentioned on the beautiful soup homepage?\n",
    "* How often does the word 'Soup' occur on the site?\n",
    "    - hint: use `.count()`\n",
    "* At what index occurs the substring 'alien video games' ?\n",
    "    - hint: use `.find()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "45\n",
      "-1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## is 'Alice' in source?\n",
    "print ('Alice' in source)\n",
    "\n",
    "## count occurences of 'Soup'\n",
    "print (source.count('Soup'))\n",
    "\n",
    "## find index of 'alien video games'\n",
    "position =  source.find('alien video games')\n",
    "print (position)\n",
    "\n",
    "## quick test to see the substring in the source variable\n",
    "## you can access strings like lists\n",
    "print (source[position:position + 20])\n",
    "\n",
    "## or the tidier version:\n",
    "print (source[position:position + len('alien video games')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Beautiful Soup\n",
    "==============\n",
    "\n",
    "* designed to make your life easier\n",
    "* many good functions for parsing html code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some examples\n",
    "=============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"bs4/download/\"><h1>Beautiful Soup</h1></a>,\n",
       " <a href=\"#Download\">Download</a>,\n",
       " <a>Documentation</a>,\n",
       " <a href=\"#HallOfFame\">Hall of Fame</a>,\n",
       " <a href=\"https://code.launchpad.net/beautifulsoup\">Source</a>,\n",
       " <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">Changelog</a>,\n",
       " <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">Discussion group</a>,\n",
       " <a href=\"zine/\">Zine</a>,\n",
       " <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">Tidelift subscription</a>,\n",
       " <a href=\"zine/\"><i>Tool Safety</i></a>,\n",
       " <a>the discussion\\ngroup</a>,\n",
       " <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file it</a>,\n",
       " <a>lxml</a>,\n",
       " <a>html5lib</a>,\n",
       " <a href=\"bs4/doc/\">Read more.</a>,\n",
       " <a name=\"Download\"><h2>Download Beautiful Soup</h2></a>,\n",
       " <a href=\"bs4/download/\">Beautiful Soup\\n4.8.0</a>,\n",
       " <a>Here\\'s\\nthe Beautiful Soup 3 documentation.</a>,\n",
       " <a>3.2.1</a>,\n",
       " <a name=\"HallOfFame\"><h2>Hall of Fame</h2></a>,\n",
       " <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>,\n",
       " <a>parse\\na page that\\'s been linked to and find a representative image</a>,\n",
       " <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a>,\n",
       " <a>migrate the Python\\nbug tracker from Sourceforge to Roundup</a>,\n",
       " <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>,\n",
       " <a>gather\\nstatewide election results</a>,\n",
       " <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a>,\n",
       " <a>TopoGrabber</a>,\n",
       " <a>the discussion\\ngroup</a>,\n",
       " <a>Launchpad</a>,\n",
       " <a>get the source\\ncode</a>,\n",
       " <a href=\"https://bugs.launchpad.net/beautifulsoup/\">file\\nbugs</a>,\n",
       " <a href=\"/source/software/BeautifulSoup/index.bhtml\">source</a>,\n",
       " <a href=\"/self/\">Leonard Richardson</a>,\n",
       " <a href=\"/self/contact.html\">contact information</a>,\n",
       " <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\"><img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/></a>,\n",
       " <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">Creative Commons License</a>,\n",
       " <a href=\"http://www.crummy.com/\">http://www.crummy.com/</a>,\n",
       " <a href=\"http://www.crummy.com/software/\">software/</a>,\n",
       " <a href=\"http://www.crummy.com/software/BeautifulSoup/\">BeautifulSoup/</a>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get bs4 object\n",
    "soup = bs4.BeautifulSoup(source)\n",
    " \n",
    "## compare the two print statements\n",
    "#print (soup)\n",
    "#print (soup.prettify())\n",
    "\n",
    "## show how to find all a tags\n",
    "soup.findAll('a')\n",
    "\n",
    "## ***Why does this not work? ***\n",
    "#soup.findAll('Soup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The last command only returns an empty list, because `Soup` is not an HTML tag. It is just a string that occours in the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some examples\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bs4/download/',\n",
       " '#Download',\n",
       " None,\n",
       " '#HallOfFame',\n",
       " 'https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'zine/',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'zine/',\n",
       " None,\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " None,\n",
       " None,\n",
       " 'bs4/doc/',\n",
       " None,\n",
       " 'bs4/download/',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " None,\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " None,\n",
       " 'http://www2.ljworld.com/',\n",
       " None,\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " '/source/software/BeautifulSoup/index.bhtml',\n",
       " '/self/',\n",
       " '/self/contact.html',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get attribute value from an element:\n",
    "## find tag: this only returns the first occurrence, not all tags in the string\n",
    "first_tag = soup.find('a')\n",
    "\n",
    "## get attribute `href`\n",
    "first_tag.get('href')\n",
    "\n",
    "## get all links in the page\n",
    "link_list = [l.get('href') for l in soup.findAll('a')]\n",
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## filter all external links\n",
    "# create an empty list to collect the valid links\n",
    "external_links = []\n",
    "\n",
    "# write a loop to filter the links\n",
    "# if it starts with 'http' we are happy\n",
    "for l in link_list:\n",
    "    if l is not None and l[:4] == 'http':\n",
    "        external_links.append(l)\n",
    "\n",
    "# this throws an error! It says something about 'NoneType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bs4/download/',\n",
       " '#Download',\n",
       " None,\n",
       " '#HallOfFame',\n",
       " 'https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'zine/',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'zine/',\n",
       " None,\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " None,\n",
       " None,\n",
       " 'bs4/doc/',\n",
       " None,\n",
       " 'bs4/download/',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " None,\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " None,\n",
       " 'http://www2.ljworld.com/',\n",
       " None,\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " '/source/software/BeautifulSoup/index.bhtml',\n",
       " '/self/',\n",
       " '/self/contact.html',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets investigate. Have a close look at the link_list:\n",
    "link_list\n",
    "\n",
    "# Seems that there are None elements!\n",
    "# Let's verify\n",
    "#print sum([l is None for l in link_list])\n",
    "\n",
    "# So there are two elements in the list that are None!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " 'http://www2.ljworld.com/',\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's filter those objects out in the for loop\n",
    "external_links = []\n",
    "\n",
    "# write a loop to filter the links\n",
    "# if it is not None and starts with 'http' we are happy\n",
    "for l in link_list:\n",
    "    if l is not None and l[:4] == 'http':\n",
    "        external_links.append(l)\n",
    "        \n",
    "external_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: The above `if` condition works because of lazy evaluation in Python. The `and` statement becomes `False` if the first part is `False`, so there is no need to ever evaluate the second part. Thus a `None` entry in the list gets never asked about its first four characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://code.launchpad.net/beautifulsoup',\n",
       " 'https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG',\n",
       " 'https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup',\n",
       " 'https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=website',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://www.nytimes.com/2007/10/25/arts/design/25vide.html',\n",
       " 'http://www.harrowell.org.uk/viktormap.html',\n",
       " 'http://www2.ljworld.com/',\n",
       " 'http://esrl.noaa.gov/gsd/fab/',\n",
       " 'https://bugs.launchpad.net/beautifulsoup/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       " 'http://www.crummy.com/',\n",
       " 'http://www.crummy.com/software/',\n",
       " 'http://www.crummy.com/software/BeautifulSoup/']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can put this in a list comprehension as well, it almost reads like \n",
    "# a sentence.\n",
    "\n",
    "[l for l in link_list if l is not None and l.startswith('http')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parsing the Tree\n",
    "================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body><h3> Test </h3><p>Hello world!</p></body>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redifining `s` without any line breaks\n",
    "s = \"\"\"<!DOCTYPE html><html><head><title>This is a title</title></head><body><h3> Test </h3><p>Hello world!</p></body></html>\"\"\"\n",
    "## get bs4 object\n",
    "tree = bs4.BeautifulSoup(s)\n",
    "\n",
    "## get html root node\n",
    "root_node = tree.html\n",
    "\n",
    "## get head from root using contents\n",
    "head = root_node.contents[0]\n",
    "\n",
    "## get body from root\n",
    "body = root_node.contents[1]\n",
    "\n",
    "## could directly access body\n",
    "tree.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a title'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.html.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Quiz:\n",
    "=====\n",
    "\n",
    "* Find the `h3` tag by parsing the tree starting at `body`\n",
    "* Create a list of all __Hall of Fame__ entries listed on the Beautiful Soup webpage\n",
    "    - hint: it is the only unordered list in the page (tag `ul`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beautiful Soup 3'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get h3 tag from body\n",
    "soup.body.h3.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li><a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\"Movable\\n Type\"</a>, a work of digital art on display in the lobby of the New\\n York Times building, uses Beautiful Soup to scrape news feeds.\\n\\n</li>, <li>Reddit uses Beautiful Soup to <a>parse\\na page that\\'s been linked to and find a representative image</a>.\\n\\n</li>, <li>Alexander Harrowell uses Beautiful Soup to <a href=\"http://www.harrowell.org.uk/viktormap.html\">track the business\\n activities</a> of an arms merchant.\\n\\n</li>, <li>The developers of Python itself used Beautiful Soup to <a>migrate the Python\\nbug tracker from Sourceforge to Roundup</a>.\\n\\n</li>, <li>The <a href=\"http://www2.ljworld.com/\">Lawrence Journal-World</a>\\nuses Beautiful Soup to <a>gather\\nstatewide election results</a>.\\n\\n</li>, <li>The <a href=\"http://esrl.noaa.gov/gsd/fab/\">NOAA\\'s Forecast\\nApplications Branch</a> uses Beautiful Soup in <a>TopoGrabber</a>, a script for\\ndownloading \"high resolution USGS datasets.\"\\n\\n</li>]\n"
     ]
    }
   ],
   "source": [
    "## use ul as entry point\n",
    "#entry = str(soup.body.ul)\n",
    "## get hall of fame list from entry point\n",
    "\n",
    "lista = []\n",
    "for l in soup.body.ul:\n",
    "    lista.append(l)\n",
    "    \n",
    "lista = lista[1:]\n",
    "print(lista)\n",
    "\n",
    "## skip the first entry\n",
    "\n",
    "## reformat into a list containing strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Advanced Example\n",
    "===============\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Scraping data science skills\n",
    "=============================\n",
    "\n",
    "- What skills are in demand for data scientists?\n",
    "- Should we have a lecture on Spark or only on MapReduce?\n",
    "\n",
    "We want to scrape the information from job advertisements for data scientists from indeed.com\n",
    "Let's scrape and find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fixed url for job postings containing data scientist\n",
    "url = 'https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco+Bay+Area%2C+CA'\n",
    "# read the website\n",
    "source = urlopen(url).read()\n",
    "# parse html code\n",
    "bs_tree = bs4.BeautifulSoup(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search yielded 2,706 hits.\n",
      "2706\n"
     ]
    }
   ],
   "source": [
    "# see how many job postings we found\n",
    "job_count_string = bs_tree.find(id = 'searchCountPages').contents[0]\n",
    "job_count_string = job_count_string.split()[-2]\n",
    "print(\"Search yielded %s hits.\" % (job_count_string))\n",
    "\n",
    "# not that job_count so far is still a string, \n",
    "# not an integer, and the , separator prevents \n",
    "# us from just casting it to int\n",
    "\n",
    "job_count_digits = [int(d) for d in job_count_string if d.isdigit()]\n",
    "job_count = np.sum([digit*(10**exponent) for digit, exponent in \n",
    "                    zip(job_count_digits[::-1], range(len(job_count_digits)))])\n",
    "\n",
    "print (job_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,706'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_count_string = bs_tree.find(id = 'searchCount').text\n",
    "job_count_string.split()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n",
      "opening https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco+Bay+Area%2C+CA&start=0\n",
      "We found a lot of jobs:  0\n"
     ]
    }
   ],
   "source": [
    "# The website is only listing 10 results per page, \n",
    "# so we need to scrape them page after page\n",
    "num_pages = int(np.ceil(job_count/10.0))\n",
    "\n",
    "base_url = 'http://www.indeed.com'\n",
    "job_links = []\n",
    "for i in range(1): #do range(num_pages) if you want them all\n",
    "    if i%10==0:\n",
    "        print (num_pages-i)\n",
    "    url = 'https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco+Bay+Area%2C+CA&start=' + str(i*10)\n",
    "    print (\"opening {}\".format(url))\n",
    "    html_page = urlopen(url).read() \n",
    "    bs_tree = bs4.BeautifulSoup(html_page)\n",
    "    job_link_area = bs_tree.find(id = 'resultsCol')\n",
    "    job_postings = job_link_area.findAll(\"div\")\n",
    "    \n",
    "\n",
    "print (\"We found a lot of jobs: \", len(job_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another Example\n",
    "================\n",
    "https://github.com/kjam/python-web-scraping-tutorial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Getting Data with an API\n",
    "=========================\n",
    "\n",
    "* API: application programming interface\n",
    "* some sites try to make your life easier\n",
    "* Twitter, New York Times, ImDB, rotten Tomatoes, Yelp, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "API keys\n",
    "=========\n",
    "\n",
    "* required for data access\n",
    "* identifies application (you)\n",
    "* monitors usage\n",
    "* limits rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "JSON\n",
    "======\n",
    "\n",
    "* JavaScript Object Notation\n",
    "* human readable\n",
    "* transmit attribute-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f39bfc3b5481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## a is a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "a = {'a': 1, 'b':2}\n",
    "s = json.dumps(a)\n",
    "a2 = json.loads(s)\n",
    "\n",
    "## a is a dictionary\n",
    "print (a)\n",
    "## vs s is a string containing a in JSON encoding\n",
    "print (s)\n",
    "## reading back the keys are now in unicode\n",
    "print (a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Twitter Example:\n",
    "================\n",
    "\n",
    "* API a bit more complicated\n",
    "* libraries make life easier\n",
    "* python-twitter\n",
    "\n",
    "https://github.com/bear/python-twitter\n",
    "\n",
    "What we are going to do is scrape Joe's twitter account, and then filter it for the interesting tweets. Defining interesting as tweets that have be re-tweeted at least 10 times. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "## define the necessary keys\n",
    "cKey = \n",
    "cSecret = \n",
    "aKey = \n",
    "aSecret = \n",
    "\n",
    "## create the api object with the twitter-python library\n",
    "api = twitter.Api(consumer_key=cKey, consumer_secret=cSecret, access_token_key=aKey, access_token_secret=aSecret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## get the user timeline with screen_name = 'stat110'\n",
    "twitter_statuses = api.GetUserTimeline(screen_name = 'gutyril')\n",
    "\n",
    "## create a data frame\n",
    "## first get a list of panda Series or dict\n",
    "pdSeriesList = [pd.Series(t.AsDict()) for t in twitter_statuses]\n",
    "\n",
    "## then create the data frame\n",
    "data = pd.DataFrame(pdSeriesList)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## filter tweets with enough retweet_count\n",
    "maybe_interesting = data[data.retweet_count>20]\n",
    "\n",
    "## get the text of these tweets\n",
    "tweet_text = maybe_interesting.text\n",
    "\n",
    "## print them out\n",
    "text = tweet_text.values\n",
    "\n",
    "for t in text:\n",
    "    print ('######')\n",
    "    print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
